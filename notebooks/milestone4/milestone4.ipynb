{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5991b5-18ea-4e77-ba3e-5e7502a61d4d",
   "metadata": {},
   "source": [
    "# Milestone 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832dbc89-079f-471d-b58e-ed60a7e15f9b",
   "metadata": {},
   "source": [
    "Contributors:\n",
    "\n",
    "- Jacob McFarlane\n",
    "- Sang Yoon Lee\n",
    "- Sukhdeep Kaur\n",
    "- Yiki Su"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58660dcd-80c8-42b6-bf7f-d7d607e71f0b",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a47ce7-637f-44b1-87af-da2c619b19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 1. Load your model here\n",
    "\n",
    "model = joblib.load('model.joblib')\n",
    "\n",
    "# 2. Define a prediction function\n",
    "def return_prediction(data):\n",
    "    # format input_data here so that you can pass it to model.predict()\n",
    "    return model.predict(data)\n",
    "\n",
    "# 3. Set up home page using basic html\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    # feel free to customize this if you like\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to our rain prediction service</h1>\n",
    "    To use this service, make a JSON post request to the /predict url with 25 climate model outputs.\n",
    "    JSON must be of the form {\"data\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]}\n",
    "    Order matters: contact the team if you are unfamiliar with the ordering of model inputs.\n",
    "    \"\"\"\n",
    "\n",
    "# 4. define a new route which will accept POST requests and return model predictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def rainfall_prediction():\n",
    "    # this extracts the JSON content we sent\n",
    "    content = request.json\n",
    "    data = content[\"data\"]\n",
    "    data = np.array(data)\n",
    "    result = return_prediction(data.reshape(1, -1)).tolist()\n",
    "    return jsonify(predictions = result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a438cbca-970c-436b-96a0-dfc5496a529f",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f0250-6d59-4cb6-9a2d-666890c8948a",
   "metadata": {},
   "source": [
    "### Screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37260fb2-8885-4345-80dd-fd7b86c3de3b",
   "metadata": {},
   "source": [
    "<img src=\"jsonresult.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11091f66-cea7-4b15-aed6-3f489ff54459",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af7a2b-9560-40d2-8ed8-6caf44dfb518",
   "metadata": {},
   "source": [
    "##### Milestone 1\n",
    "\n",
    "> In this milestone, we tried to read in some big datasest and run some operations on the local machines. We realized how difficult and inefficient to work with large datasets on local machines. Especially for machines that don't have a high configurations, machines crash easily due to high memory usage. We explored some other ways to reduce the time it takes and the memory it requires to run operations in a large datasets or to read in a dataframe, such as changing the data types of the columns, loading and processing data in chunks, Dask, parquet files, feather files and so on. \n",
    "\n",
    "##### Milestone 2\n",
    "\n",
    "> In this milestone, we started to enter the journey of web and cloud computing in AWS. We started the journey with setting up and connecting to the EC2 instance and S3 bucket. We learned how to transfer files from local to the cloud or from the cloud to local, and file transfers between the EC2 instance and S3 bucket as well. We also explored ways to create shared folder in the EC2 instance among group members. In this milestone, we also got to work on the dataset we had in milestone 1 to process some data wrangling on the JupyterHub on EC2 instance. It was much faster and efficient to run wrangling codes on large datasets on the JupyterHub compared to running them on the local machine since we chose some high configuration for the instance.\n",
    "\n",
    "##### Milestone 3\n",
    "\n",
    "> In this milestone, we first spun up an EMR cluster in AWS to try distributed computing with PySpark. We read in the data, do train-test data splitting, perform EDA on the training set and train a model with default parameters on the EC2 instance. And then we used the PySpark machine on EMR cluster to find out the best-performing hyperparameters. Hyperparameter tuning for models that use large training datasets could be very time consuming and computational expensive, PySpark distributes the cross-validation tasks which makes it more efficient. Based on the hyperparameter tuning result, we trained a hyperparameter-tuned model in the EC2 instance and saved the model in our S3 bucket for later deployment. \n",
    "\n",
    "##### Milestone 4\n",
    "\n",
    "> In this milestone, we learned how to deploy the model that we developed in the last milestone using REST API to make it accessible by others. We built a simple Flask web framework, which takes in POST request, processes it through the machine learning model and generates a prediction result for the user. This was set up in the app.py script, which is loaded to Flask. Finally, we ran the Flask application in a screen session to make sure that our service would not be terminated after we exit the EC2 instance connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c226f-109c-4bad-8e15-b18cf39e867e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
